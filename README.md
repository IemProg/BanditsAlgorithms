# BanditsAlgorithms
A repo containing the implementation of well known algorithms for Bandits such as UCB, UCB with Thompson Sampling and an example of creating a recommendation system
